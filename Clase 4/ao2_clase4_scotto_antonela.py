# -*- coding: utf-8 -*-
"""AO2_Clase4_Scotto_Antonela

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17bSAdRM54_9cbvRjVJ2keiW2IZc6baZ4

#Actividad 1
Es importante que realices todas las actividades, estas te permitirán saber como vas avanzando y podrás reflexionar sobre la teoría si te queda alguna duda de cómo hacer esta actividad por favor consultanos en el campus.
El objetivo de esta actividad es encontrar una posible relación o linealidad entre el PIB (Producto Interno Bruto) de diferentes países del mundo. Los datos fueron descargados del sitio del Banco Mundial.

El método que seguiremos es el siguiente:

*   Preparación y organización de datos
*   Exploración de los datos
*   Modelado de datos

Debes descargar el archivo BancoMundial.ipynb del repositorio de la materia. Realiza las
pruebas necesarias para identificar ¿qué países o combinación de países tendrían la
mayor relación con el PIB (Producto Interno Bruto) mundial?
"""

# Librerías principales
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix

df_pbi = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AA clase 4/pib_banco_mundial_50.csv', delimiter= ';')
df_pbi.head()

df_pbi.info()
df_pbi.describe()
df_pbi.columns
df_pbi.isnull().sum()

# Se elimina las columnas que no son relevantes para el estudio. En este caso la columna "Country Code"

df_pbi.drop('Country Code', axis=1, inplace=True)
df_pbi.head()

# Se traspone el df por que es mejor para la interpretación.
df_pbi_t = df_pbi.set_index('Country Name').T
df_pbi_t.head()

#Transformación de los datos a tipo numerico
df_pbi_t= df_pbi_t.apply(pd.to_numeric)
df_pbi_t.tail()

# Seleccionar los top 5 países más correlacionados con el PIB mundial
corr = df_pbi_t.corr()['World'].drop('World').sort_values(ascending=False)
top5_paises = corr.head(5).index.tolist()
print("Top 5 países más correlacionados:", top5_paises)

# Dataset limpio (sin NaN y con índices alineados)
df_modelo = df_pbi_t[top5_paises + ['World']].dropna()

X = df_modelo[top5_paises]
y = df_modelo['World']

# Dividimos en entrenamiento y prueba (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Creamos y entrenamos el modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Predicciones
y_pred = model.predict(X_test)

print("R² entrenamiento:", model.score(X_train, y_train))
print("R² prueba:", model.score(X_test, y_test))

# --- Importancia de cada país ---
coef_df = pd.DataFrame({
    "País": X.columns,
    "Coeficiente": model.coef_
}).sort_values(by="Coeficiente", key=abs, ascending=False)

print("\nImportancia de países en el modelo:")
print(coef_df)

# --- Gráfico real vs predicho ---
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.xlabel("PIB mundial real")
plt.ylabel("PIB mundial predicho")
plt.title("Regresión lineal - Top 5 países")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color="red", linestyle="--")
plt.show()

"""###Analisis acitvidad 1

Con el analisis de correlación y la regresion lineal multiple podemos  inferir que los PIB de los paises seleccionados TOP5 tienen una fuerte relación con el PIB mundial. R2 siendo perfecto en 1.0 explica la variacion. Entonces podemos decir que estos paises siguien la tendencia global y estan fuertemente alineadas.

#Actividad 2
Te proponemos entonces que realices ahora la siguiente actividad con los conceptos leídos anteriormente.

Para esta actividad se ha creado un archivo usuarios_win_mac_lin.csv con datos de entrada a modo de ejemplo para clasificar si el usuario que visita un sitio web usa como sistema operativo Windows, Macintosh o Linux.

Nuestra información de entrada son 4 características que se tomó de una web que utiliza Google Analytics y son:

- Duración de la visita en Segundos
- Cantidad de Páginas Vistas durante la Sesión
- Cantidad de Acciones del usuario (click, scroll, uso de checkbox, sliders,etc)
- Suma del Valor de las acciones (cada acción lleva asociada una valoración
de importancia)

Como la salida es discreta, asignaremos los siguientes valores a las etiquetas:
- 0 – Windows
- 1 – Macintosh
- 2 -Linux

Recorre la ejecución de cada una de las celdas de la Jupyter Notebook
Regresion_logistica.ipynb que se encuentra en el repositorio de la materia y verifica que sucede cuando realizamos la clasificación (o predicción) de tipo de usuario cuando se altera los valores de las variables.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

#Carga de datos
df_usuarios = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AA clase 4/usuarios_win_mac_lin.csv')

df_usuarios.info()
df_usuarios.describe()
df_usuarios.columns
df_usuarios.isnull().sum()

# Vista inicial
print("Primeras filas del dataset:")
print(df_usuarios.head())

print("\nDistribución de clases:")
print(df_usuarios['clase'].value_counts())

df_usuarios.tail(170)

#histograma sobre la distribucion de clases
sns.countplot(x='clase', data=df_usuarios)
plt.show()

#Creacion de arreglos.
X_data = df_usuarios[['duracion', 'paginas', 'acciones', 'valor']]
Y_data = df_usuarios['clase']

X = np.array(X_data)
Y = np.array(Y_data)

# División en entrenamiento y prueba
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)

# Entrenamiento del modelo
modelo = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')
modelo.fit(X_train, Y_train)

# Evaluación del modelo (usamos los datos de testeo)
Y_pred = modelo.predict(X_test)

# Calcula la precisión (accuracy), % de predicciones correctas que hizo el modelo
print("Precisión del modelo:", round(accuracy_score(Y_test, Y_pred), 2))

#regresión contraida
print("Matriz de confusión:")
print(confusion_matrix(Y_test, Y_pred))

print("Matriz de confusión:")
cm = confusion_matrix(Y_test, Y_pred, labels=modelo.classes_)

# Mapa de calor para visualizar mejor
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=modelo.classes_, yticklabels=modelo.classes_)
plt.title("Matriz de confusión - Clasificación de usuarios")
plt.xlabel("Predicción")
plt.ylabel("Valor real")
plt.show()

print("\nReporte de clasificación:")
print(classification_report(Y_test, Y_pred))

# Cantidad de usuarios
n_usuarios = 150

# Generar datos aleatorios
duracion = np.random.randint(1, 3500, size=n_usuarios)
paginas = np.random.randint(1, 10, size=n_usuarios)
acciones = np.random.randint(1, 60, size=n_usuarios)
valor = np.random.randint(1, 400, size=n_usuarios)
clase = np.random.choice([0, 1, 2], size=n_usuarios)

# DataFrame completo
df = pd.DataFrame({
    'duracion': duracion,
    'paginas': paginas,
    'acciones': acciones,
    'valor': valor,
    'clase': clase
})

# Predicción
predicciones = modelo.predict(df[['duracion', 'paginas', 'acciones', 'valor']].values)
df['prediccion'] = predicciones

# Matriz de confusión
cm = confusion_matrix(df['clase'], df['prediccion'])

# Mostrar matriz con colores
plt.figure(figsize=(6,5))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Matriz de Confusión")
plt.colorbar()
plt.xticks([0,1,2], ['0','1','2'])
plt.yticks([0,1,2], ['0','1','2'])
plt.xlabel('Predicción')
plt.ylabel('Clase verdadera')

# Mostrar valores
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha='center', va='center', color='red')

plt.show()


# Reporte de clasificación
print("Reporte de Clasificación:\n")
print(classification_report(df['clase'], df['prediccion'], target_names=['0','1','2']))

# Precisión global del modelo
precision_global = accuracy_score(df['clase'], df['prediccion'])
print(f"Precisión global del modelo: {precision_global:.2f}")

"""#Actividad 3

Objetivo:
Construir un modelo de regresión lineal múltiple para predecir la presión arterial de las personas en función de diferentes variables explicativas.

Dataset: dataset_regresion_multiple.csv
Columnas incluidas:

edad: Edad de la persona (años).

horas_ejercicio: Horas semanales de ejercicio físico.

peso: Peso corporal (kg).

estres: Nivel de estrés percibido (1 a 10).

ingresos: Ingreso anual en pesos.

horas_tv: Horas semanales viendo televisión.

presion_arterial: Variable objetivo (mmHg).
"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AA clase 4/dataset_regresion_multiple.csv', sep=';')
df.head()

df.info()
df.describe()
df.columns
df.isnull().sum()

# Matriz de correlación
correlacion = df.corr()
print(correlacion)

# Mapa de calor de correlación
plt.figure(figsize=(10,8))
sns.heatmap(correlacion, annot=True, cmap='coolwarm')
plt.show()

# Definir variables predictoras y variable objetivo
X = df[['edad', 'horas_ejercicio', 'peso', 'estres', 'ingresos', 'horas_tv']]
y = df['presion_arterial']

# Separar datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ajustar modelo
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Predicciones
y_pred = modelo.predict(X_test)

# Coeficientes
coef = pd.DataFrame({'Variable': X.columns, 'Coeficiente': modelo.coef_})
print(coef)

# Intercepto
print("Intercepto:", modelo.intercept_)

# Ordenar variables por correlación absoluta con la presión arterial
correlacion_presion = correlacion['presion_arterial'].drop('presion_arterial').abs().sort_values(ascending=False)
print(correlacion_presion)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# calculamos las métricas
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"R²: {r2:.3f}")
print(f"RMSE: {rmse:.3f}")
print(f"MAE: {mae:.3f}")